{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "<br></br>\n",
    "Take me to the [code](https://github.com/AMoazeni/Word-Count/blob/master/Code/Word%20Count.py) for Word Counting!\n",
    "\n",
    "<br></br>\n",
    "Counting the occurrence of words in a document is difficult. Let's solve this problem with a few lines of Python code. Use the provided 'WordCount' function which requires a file path input (document_name) and the minimum frequency (min_occurrence) of words you're looking for. This code can be used to generate insights from documents.\n",
    "\n",
    "<br></br>\n",
    "Once you download this repository, you can drop and '.txt' documents into the 'Data' folder and use the following code to analyze it. Here are some insights derived from the examples documents in the 'Data' folder.\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Word-Count/master/Jupyter%20Notebook/Images/01%20-%20Counting.gif\" width=40% alt=\"Counting\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Popular CEO Names\n",
    "\n",
    "Analyzing Fortune 1000 companies, it turns out that you have the best chances of becoming CEO if your name is David, James, or John.\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Word-Count/master/Jupyter%20Notebook/Images/02%20-%20CEO%20Names.png\" width=70% alt=\"CEO-Names\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Software Engineering Jobs\n",
    "\n",
    "Apparently Software Engineer jobs really care about Technical Skills, Scale, and Communication. Good to know when writing your resume.\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Word-Count/master/Jupyter%20Notebook/Images/03%20-%20Software%20Engineer.png\" width=70% alt=\"Software-Engineer\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Shakespeare Word Count\n",
    "\n",
    "It turns out 'father', 'think', and 'queen' are used quite often in Shakespeare's writing. This can give us insight into his personality. \n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Word-Count/master/Jupyter%20Notebook/Images/04%20-%20Shakespeare.png\" width=70% alt=\"Shakespeare\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Sherlock Holmes\n",
    "\n",
    "Interesting that Sherlock Holmes likes to investigate mysteries at night and morning time. \n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Word-Count/master/Jupyter%20Notebook/Images/05%20-%20Sherlock.png\" width=70% alt=\"Sherlock\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Code\n",
    "\n",
    "1. Install [Anaconda](https://www.anaconda.com/download/).\n",
    "2. Download this repository and navigate to it.\n",
    "3. Copy the '.txt' file you want to analyze into the 'Data' folder.\n",
    "4. Open the 'Word Count.ipynb' file with Jupyter Notebook.\n",
    "5. Type your '.txt' file name into the 'WordCount' function.\n",
    "6. Click 'Run' to step through the code.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "```shell\n",
    "$ git clone https://github.com/AMoazeni/Word-Count.git\n",
    "$ cd Word-Count\n",
    "```\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Happy Coding!\n",
    "\n",
    "Check out [AMoazeni's Github](https://github.com/AMoazeni/) for more Data Science, Machine Learning, and Robotics repositories.\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Word-Count/master/Jupyter%20Notebook/Images/06%20-%20Cat%20Typing.gif\" width=40% alt=\"Cat-Typing\"></div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Correct Anaconda Environment\n",
    "conda create -n chatbot python=3.5 anaconda\n",
    "source activate chatbot\n",
    "pip install tensorflow==1.0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatBot with Deep NLP\n",
    "# PART 1 - DATA PREPROCESSING\n",
    "\n",
    "\n",
    "\n",
    "# Importing libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "\n",
    "\n",
    "# Importing the dataset and split by lines of dataset\n",
    "lines = open('../Data/movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
    "conversations = open('../Data/movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
    "\n",
    "\n",
    "# Creating a dictionary that maps each line and its id\n",
    "# '_line' is a temporary variable only used in the loop\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    \n",
    "    # Only accept lines with 5 elements to avoid shifting issues\n",
    "    if len(_line) == 5:\n",
    "        # Create a key from the first element (line ID) and value from last element (words)\n",
    "        id2line[_line[0]] = _line[4]\n",
    "\n",
    "\n",
    "# Creating a list of all of the conversations\n",
    "conversations_ids = []\n",
    "\n",
    "# Ignore last row which is empty\n",
    "for conversation in conversations[:-1]:\n",
    "    \n",
    "    # [-1] takes the last element, [1:-1] removes the first and last element (square brakets)\n",
    "    # Remove single quotes and remove spaces\n",
    "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
    "    # Split result array by commas, and append to conversation_ids\n",
    "    conversations_ids.append(_conversation.split(','))\n",
    "\n",
    "\n",
    "# Getting separately the questions and the answers\n",
    "questions = []\n",
    "answers = []\n",
    "for conversation in conversations_ids:\n",
    "    for i in range(len(conversation) - 1):\n",
    "        questions.append(id2line[conversation[i]])\n",
    "        answers.append(id2line[conversation[i+1]])\n",
    "\n",
    "\n",
    "# Doing a first cleaning of the texts\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"let's\", \"let us\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"c'mon\", \"come on\", text)\n",
    "    text = re.sub(r\"didn't\", \"did not\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"haven't\", \"have not\", text)\n",
    "    text = re.sub(r\"'em\", \"them\", text)\n",
    "    text = re.sub(r\"'in\", \"ing\", text)\n",
    "    text = re.sub(r\"'cause\", \"because\", text)\n",
    "    text = re.sub(r\"c'mere\", \"come here\", text)\n",
    "    text = re.sub(r\"wasn't\", \"was not\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# Cleaning the questions\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "\n",
    "# Cleaning the answers\n",
    "clean_answers = []\n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))\n",
    "\n",
    "\n",
    "# Creating a dictionary that maps each word to its number of occurrences\n",
    "word2count = {}\n",
    "for question in clean_questions:\n",
    "    for word in question.split():\n",
    "        \n",
    "        # Check if first apperance of word\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        \n",
    "        # Increment number of occurance\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "            \n",
    "# Count words occurance in answers           \n",
    "for answer in clean_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "\n",
    "\n",
    "\n",
    "# Creating two dictionaries that map the questions words and the answers words to a unique integer (tokenization)\n",
    "# 5% threshold of 20,000 words is 20\n",
    "threshold_questions = 20\n",
    "questionswords2int = {}\n",
    "word_number = 0\n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold_questions:\n",
    "        questionswords2int[word] = word_number\n",
    "        word_number += 1\n",
    "\n",
    "\n",
    "threshold_answers = 20\n",
    "answerswords2int = {}\n",
    "word_number = 0\n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold_answers:\n",
    "        answerswords2int[word] = word_number\n",
    "        word_number += 1\n",
    "\n",
    "\n",
    "\n",
    "# Adding the last tokens to these two dictionaries\n",
    "# <PAD> Empty space filling\n",
    "# <EOS> End of string\n",
    "# <OUT> Word replacement for uncommon words filtered out by the threshold\n",
    "# <SOS> Start of string\n",
    "\n",
    "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
    "for token in tokens:\n",
    "    questionswords2int[token] = len(questionswords2int) + 1\n",
    "for token in tokens:\n",
    "    answerswords2int[token] = len(answerswords2int) + 1\n",
    "\n",
    "\n",
    "# Creating the inverse dictionary of the answerswords2int dictionary, used for decoding\n",
    "answersints2word = {w_i: w for w, w_i in answerswords2int.items()}\n",
    "\n",
    "\n",
    "# Adding the End Of String token to the end of every answer\n",
    "for i in range(len(clean_answers)):\n",
    "    clean_answers[i] += ' <EOS>'\n",
    "\n",
    "\n",
    "# Translating all the questions and the answers into integers\n",
    "# and Replacing all the words that were filtered out by <OUT> \n",
    "questions_into_int = []\n",
    "for question in clean_questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questionswords2int:\n",
    "            ints.append(questionswords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(questionswords2int[word])\n",
    "    questions_into_int.append(ints)\n",
    "answers_into_int = []\n",
    "for answer in clean_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answerswords2int:\n",
    "            ints.append(answerswords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(answerswords2int[word])\n",
    "    answers_into_int.append(ints)\n",
    "\n",
    "\n",
    "# Sorting questions and answers by the length of questions\n",
    "sorted_clean_questions = []\n",
    "sorted_clean_answers = []\n",
    "\n",
    "# Easier to train shorter questions first, start with 25 word long questions\n",
    "for length in range(1, 25 + 1):\n",
    "    # 'enumerate' get a couple containing (index, question)\n",
    "    for i in enumerate(questions_into_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_clean_questions.append(questions_into_int[i[0]])\n",
    "            sorted_clean_answers.append(answers_into_int[i[0]])\n",
    "            \n",
    "\n",
    "print(\"Finished Data Preprocessing Training!!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatBot with Deep NLP\n",
    "# PART 2 - BUILDING THE SEQ2SEQ MODEL\n",
    "\n",
    "\n",
    "# Creating placeholders for the inputs and the targets (answers)\n",
    "def model_inputs():\n",
    "    \n",
    "    # placeolder('type of data', 'dimension of input', 'name of placeholder')\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name = 'input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name = 'target')\n",
    "    \n",
    "    # 'lr' - Learning Rate\n",
    "    lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "    \n",
    "    # 'keep_prob' - Dropout Rate (avoids overfitting)\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    \n",
    "    return inputs, targets, lr, keep_prob\n",
    "\n",
    "\n",
    "\n",
    "# Preprocessing the targets\n",
    "def preprocess_targets(targets, word2int, batch_size):\n",
    "    \n",
    "    # Create <SOS> token column for the beginning of target\n",
    "    left_side = tf.fill([batch_size, 1], word2int['<SOS>'])\n",
    "    \n",
    "    # Create Last Column which is all the answers, exclude last column token whichis not needed for decoding\n",
    "    right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
    "    \n",
    "    # '1' means horizontal concatenation\n",
    "    preprocessed_targets = tf.concat([left_side, right_side], 1)\n",
    "    \n",
    "    return preprocessed_targets\n",
    "\n",
    "\n",
    "\n",
    "# Creating the Encoder RNN\n",
    "\n",
    "# 'rnn_inputs' - Model inputs which inludes learning_rate, keep_prop etc\n",
    "# 'rnn_size' - Number of input tensors in the layers\n",
    "# 'keep_prop' - Used for Dropout Regularization\n",
    "# 'sequence_length' - List of length of questions in each bach\n",
    "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    \n",
    "    # Create a simple LSTM class\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    \n",
    "    # Dropout wrapper class for LSTM, typically 20% of neurons are deactivated to mitigate overfitting\n",
    "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    \n",
    "    # Multi RNN layer setup\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "    \n",
    "    # Encoder State - Creates a dynamic version of a bi-directional RNN\n",
    "    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell,\n",
    "                                                                    cell_bw = encoder_cell,\n",
    "                                                                    sequence_length = sequence_length,\n",
    "                                                                    inputs = rnn_inputs,\n",
    "                                                                    dtype = tf.float32)\n",
    "    return encoder_state\n",
    "\n",
    "\n",
    "\n",
    "# Decoding the training set\n",
    "    \n",
    "# 'encoder_state' - Recieve this class as an input to decode\n",
    "# 'decoder_cell' - Cel in the RNN of the decoder\n",
    "# 'decoder_embedded_input' - Inputs that have embedding enabled (mapping from objects like words to a vector of unique real numbers)\n",
    "# 'decoding_scope' - It's a data structure that wraps your data called 'variable_scope'\n",
    "# 'output_function' - Function used to return outputs at the end\n",
    "# 'keep_prob' - Dropout regularization\n",
    "# 'batch_size' - Working with batches\n",
    "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    \n",
    "    # Initialize 3D matrix containing zeros\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    \n",
    "    # Get all the Attention Function attributes from the seq2seq library\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "    \n",
    "    # Attention Function decoder\n",
    "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              name = \"attn_dec_train\")\n",
    "    \n",
    "    # Get the output and final state of the decoder\n",
    "    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                              training_decoder_function,\n",
    "                                                                                                              decoder_embedded_input,\n",
    "                                                                                                              sequence_length,\n",
    "                                                                                                              scope = decoding_scope)\n",
    "    \n",
    "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
    "    \n",
    "    return output_function(decoder_output_dropout)\n",
    "\n",
    "\n",
    "\n",
    "# Decoding the test/validation set\n",
    "    \n",
    "# Same as above 'decode_training_set' function except use 'attention_decoder_fn_inference' to logically infer context\n",
    "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
    "                                                                              encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              decoder_embeddings_matrix,\n",
    "                                                                              sos_id,\n",
    "                                                                              eos_id,\n",
    "                                                                              maximum_length,\n",
    "                                                                              num_words,\n",
    "                                                                              name = \"attn_dec_inf\")\n",
    "    test_predictions, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                                test_decoder_function,\n",
    "                                                                                                                scope = decoding_scope)\n",
    "    return test_predictions\n",
    "\n",
    "\n",
    "\n",
    "# Creating the Decoder RNN\n",
    "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size):\n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "        \n",
    "        # Initialize weights with a standard deviation = 0.1\n",
    "        weights = tf.truncated_normal_initializer(stddev = 0.1)\n",
    "        \n",
    "        # Set up biases\n",
    "        biases = tf.zeros_initializer()\n",
    "        \n",
    "        # Fully connected output function\n",
    "        output_function = lambda x: tf.contrib.layers.fully_connected(x,\n",
    "                                                                      num_words,\n",
    "                                                                      None,\n",
    "                                                                      scope = decoding_scope,\n",
    "                                                                      weights_initializer = weights,\n",
    "                                                                      biases_initializer = biases)\n",
    "        \n",
    "        # Get training predictions\n",
    "        training_predictions = decode_training_set(encoder_state,\n",
    "                                                   decoder_cell,\n",
    "                                                   decoder_embedded_input,\n",
    "                                                   sequence_length,\n",
    "                                                   decoding_scope,\n",
    "                                                   output_function,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size)\n",
    "        \n",
    "        # Test training predictions\n",
    "        decoding_scope.reuse_variables()\n",
    "        test_predictions = decode_test_set(encoder_state,\n",
    "                                           decoder_cell,\n",
    "                                           decoder_embeddings_matrix,\n",
    "                                           word2int['<SOS>'],\n",
    "                                           word2int['<EOS>'],\n",
    "                                           sequence_length - 1,\n",
    "                                           num_words,\n",
    "                                           decoding_scope,\n",
    "                                           output_function,\n",
    "                                           keep_prob,\n",
    "                                           batch_size)\n",
    "    \n",
    "    return training_predictions, test_predictions\n",
    "\n",
    "\n",
    "\n",
    "# Building the seq2seq model\n",
    "\n",
    "# inputs - Question asked from the chatbot\n",
    "# targets - Answer to the question\n",
    "# sequence_length - How long the sequence should be\n",
    "# answers_num_words - Number of words in answers\n",
    "# questions_num_words - Number of words in questions\n",
    "# encoder_embedding_size - Number of dimensions in encoder\n",
    "# decoder_embedding_size - Number of dimensions in decoder\n",
    "# questionswords2int - Dictionary of words\n",
    "\n",
    "def seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionswords2int):\n",
    "    encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs,\n",
    "                                                              answers_num_words + 1,\n",
    "                                                              encoder_embedding_size,\n",
    "                                                              initializer = tf.random_uniform_initializer(0, 1))\n",
    "    \n",
    "    # Define the Encoder RNN\n",
    "    encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "    \n",
    "    # Words2Int preprocessed targets\n",
    "    preprocessed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n",
    "    \n",
    "    # Dimensions of the Embedding Matrix - initialize with random uniform distribution\n",
    "    decoder_embeddings_matrix = tf.Variable(tf.random_uniform([questions_num_words + 1, decoder_embedding_size], 0, 1))\n",
    "    \n",
    "    # Get decoder embedded input\n",
    "    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n",
    "    \n",
    "    # Get predictions\n",
    "    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input,\n",
    "                                                         decoder_embeddings_matrix,\n",
    "                                                         encoder_state,\n",
    "                                                         questions_num_words,\n",
    "                                                         sequence_length,\n",
    "                                                         rnn_size,\n",
    "                                                         num_layers,\n",
    "                                                         questionswords2int,\n",
    "                                                         keep_prob,\n",
    "                                                         batch_size)\n",
    "    return training_predictions, test_predictions\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatBot with Deep NLP\n",
    "# PART 3 - TRAINING THE SEQ2SEQ MODEL\n",
    "\n",
    "\n",
    "\n",
    "# Setting the Hyperparameters\n",
    "\n",
    "# Itterations of the entire training process (100+ for good results, no lower than 50)\n",
    "epochs = 1\n",
    "\n",
    "# Batches of questions used in training (128 for faster runtime)\n",
    "batch_size = 64\n",
    "\n",
    "# Size of Recurrent Neural Network\n",
    "rnn_size = 512\n",
    "\n",
    "# Number of Encoder and Decoder layers\n",
    "num_layers = 3\n",
    "\n",
    "# Number of columns in the Embedding Matrix\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "\n",
    "# Learning Rate too high means model trained too fast and can't speak properly, too low will be really slow\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Percentage by which learning rate is decayed over iterations, '1' means no decay\n",
    "learning_rate_decay = 0.9\n",
    "\n",
    "# Cap the the minimum learning rate\n",
    "min_learning_rate = 0.0001\n",
    "\n",
    "# Dropout reguralization, probability of neurons to be ignored during training (use 20% for input units, 50% for hidden layers)\n",
    "keep_probability = 0.5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a TensorFlow session\n",
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Loading the model inputs\n",
    "inputs, targets, lr, keep_prob = model_inputs()\n",
    "\n",
    "# Setting the sequence length TO maximum sequence length (25 word sequence)\n",
    "sequence_length = tf.placeholder_with_default(25, None, name = 'sequence_length')\n",
    "\n",
    "# Getting the shape of the inputs tensor\n",
    "input_shape = tf.shape(inputs)\n",
    "\n",
    "\n",
    "# Get the training and test predictions reshaped into the correct size\n",
    "training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]),\n",
    "                                                       targets,\n",
    "                                                       keep_prob,\n",
    "                                                       batch_size,\n",
    "                                                       sequence_length,\n",
    "                                                       len(answerswords2int),\n",
    "                                                       len(questionswords2int),\n",
    "                                                       encoding_embedding_size,\n",
    "                                                       decoding_embedding_size,\n",
    "                                                       rnn_size,\n",
    "                                                       num_layers,\n",
    "                                                       questionswords2int)\n",
    "\n",
    "\n",
    "# Setting up the Loss Error, the Optimizer and Gradient Clipping (Cap gradients to prevent expanging/disappearing gradient)\n",
    "with tf.name_scope(\"optimization\"):\n",
    "    loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,\n",
    "                                                  targets,\n",
    "                                                  tf.ones([input_shape[0], sequence_length]))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = optimizer.compute_gradients(loss_error)\n",
    "    clipped_gradients = [(tf.clip_by_value(grad_tensor, -5., 5.), grad_variable) for grad_tensor, grad_variable in gradients if grad_tensor is not None]\n",
    "    optimizer_gradient_clipping = optimizer.apply_gradients(clipped_gradients)\n",
    "\n",
    "\n",
    "# Padding the sequences with the <PAD> token - makes sure the length of the question sequence is the same as the answer sequence\n",
    "# Question - [ 'Who', 'are',  'you', <PAD>, <PAD>, <PAD>, <PAD>]\n",
    "# Answer -   [<SOS> 'I',  'am',  'a',  'bot',  '.', <EOS>, <PAD>]\n",
    "def apply_padding(batch_of_sequences, word2int):\n",
    "    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
    "    return [sequence + [word2int['<PAD>']] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]\n",
    "\n",
    "\n",
    "# Splitting the data into batches of questions and answers\n",
    "def split_into_batches(questions, answers, batch_size):\n",
    "    for batch_index in range(0, len(questions) // batch_size):\n",
    "        start_index = batch_index * batch_size\n",
    "        questions_in_batch = questions[start_index : start_index + batch_size]\n",
    "        answers_in_batch = answers[start_index : start_index + batch_size]\n",
    "        padded_questions_in_batch = np.array(apply_padding(questions_in_batch, questionswords2int))\n",
    "        padded_answers_in_batch = np.array(apply_padding(answers_in_batch, answerswords2int))\n",
    "        yield padded_questions_in_batch, padded_answers_in_batch\n",
    "\n",
    "\n",
    "# Splitting the questions and answers into training and validation sets\n",
    "training_validation_split = int(len(sorted_clean_questions) * 0.15)\n",
    "training_questions = sorted_clean_questions[training_validation_split:]\n",
    "training_answers = sorted_clean_answers[training_validation_split:]\n",
    "validation_questions = sorted_clean_questions[:training_validation_split]\n",
    "validation_answers = sorted_clean_answers[:training_validation_split]\n",
    "\n",
    "\n",
    "# Begin Training\n",
    "\n",
    "# Check training loss every 100 batches\n",
    "batch_index_check_training_loss = 100\n",
    "\n",
    "# Check validation loss every half number of batches\n",
    "batch_index_check_validation_loss = ((len(training_questions)) // batch_size // 2) - 1\n",
    "\n",
    "# Compute sum of losses every 100 batches\n",
    "total_training_loss_error = 0\n",
    "\n",
    "# List of validation loss errors - check all losses, pick minimum\n",
    "list_validation_loss_error = []\n",
    "\n",
    "# Each time loss is not improved, stop training. Make it last all the way through epochs by choosing a number higher than epochs (100)\n",
    "early_stopping_check = 0\n",
    "early_stopping_stop = 1000\n",
    "\n",
    "# Save weights\n",
    "# For Windows users, replace this line of code by: checkpoint = \"./chatbot_weights.ckpt\"\n",
    "checkpoint = \"chatbot_weights.ckpt\"\n",
    "\n",
    "# Run the TensorFlow session\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "# Begin Training loop for all epochs\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    # Algorithm for one epoch\n",
    "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_questions, training_answers, batch_size)):\n",
    "        \n",
    "        # Measure run time\n",
    "        starting_time = time.time()\n",
    "        \n",
    "        # Measure trining loss error\n",
    "        _, batch_training_loss_error = session.run([optimizer_gradient_clipping, loss_error], {inputs: padded_questions_in_batch,\n",
    "                                                                                               targets: padded_answers_in_batch,\n",
    "                                                                                               lr: learning_rate,\n",
    "                                                                                               sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                                               keep_prob: keep_probability})\n",
    "        \n",
    "        # Add patch error to total error\n",
    "        total_training_loss_error += batch_training_loss_error\n",
    "        \n",
    "        # Measure batch time\n",
    "        ending_time = time.time()\n",
    "        batch_time = ending_time - starting_time\n",
    "        \n",
    "        # Average of training loss error of 100 batches\n",
    "        if batch_index % batch_index_check_training_loss == 0:\n",
    "            print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epoch,\n",
    "                                                                                                                                       epochs,\n",
    "                                                                                                                                       batch_index,\n",
    "                                                                                                                                       len(training_questions) // batch_size,\n",
    "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n",
    "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n",
    "            total_training_loss_error = 0\n",
    "        \n",
    "        \n",
    "        # Average validation loss error at halfway and end of epoch\n",
    "        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n",
    "            total_validation_loss_error = 0\n",
    "            starting_time = time.time()\n",
    "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_questions, validation_answers, batch_size)):\n",
    "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n",
    "                                                                       targets: padded_answers_in_batch,\n",
    "                                                                       lr: learning_rate,\n",
    "                                                                       sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                       keep_prob: 1})\n",
    "                total_validation_loss_error += batch_validation_loss_error\n",
    "            ending_time = time.time()\n",
    "            batch_time = ending_time - starting_time\n",
    "            average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\n",
    "            print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "            list_validation_loss_error.append(average_validation_loss_error)\n",
    "            if average_validation_loss_error <= min(list_validation_loss_error):\n",
    "                print('I speak better now!!')\n",
    "                early_stopping_check = 0\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session, checkpoint)\n",
    "            else:\n",
    "                print(\"Sorry I do not speak better, I need to practice more.\")\n",
    "                early_stopping_check += 1\n",
    "                if early_stopping_check == early_stopping_stop:\n",
    "                    break\n",
    "    \n",
    "    \n",
    "    # Check for early stopping\n",
    "    if early_stopping_check == early_stopping_stop:\n",
    "        print(\"My apologies, I cannot speak better anymore. This is the best I can do.\")\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"Finished Training!!\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
